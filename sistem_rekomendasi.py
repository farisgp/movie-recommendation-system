# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10k4W3qXGU81mD3tVTF5FLznt-2cUDzdV

**Import Dataset**
"""

from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import time
from tqdm import tqdm
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import silhouette_score
from sklearn.mixture import GaussianMixture
from sklearn.metrics import precision_score, recall_score
import warnings
warnings.filterwarnings("ignore")

# Upload kaggle.json yang didapatkan dari akun Kaggle
from google.colab import files
files.upload()  # Pilih file kaggle.json

# Buat direktori dan ubah izin file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset
!kaggle datasets download -d shubhammehta21/movie-lens-small-latest-dataset

# Unzip dataset dan hapus file zip
!unzip movie-lens-small-latest-dataset.zip

"""Dataset yang digunakan dalam proyek ini berasal dari [Kaggle - Movie Lens Small Latest Dataset](https://www.kaggle.com/datasets/shubhammehta21/movie-lens-small-latest-dataset)

**Data Understanding**

**Data Loading**
"""

movies = pd.read_csv('movies.csv')
ratings = pd.read_csv('ratings.csv')

# Melihat missing value dan duplikasi data pada tabel movie
print("missing value movie: ", movies.isnull().sum())
print("jumlah duplikat movie: ", movies.duplicated().sum())

# Melihat missing value dan duplikasi data pada tabel rating
print("missing value rating: ", ratings.isnull().sum())
print("jumlah duplikat rating: ", ratings.duplicated().sum())

"""Dataset ini berisi informasi terkait data movies seperti judul film dan genre, data links berisi informasi koneksi antara movieId di dataset ini dengan ID di basis data film eksternal (IMDB dan TMDB), data tags berisi tag atau anotasi yang diberikan pengguna pada film, dan data ratings berisi terkait rating dari pengguna terhadap film. Pada proyek kali ini akan menggunakan hanya dataset pada data movies, dan ratings. Pada kedua dataset tersebut tidak terdapat missing value setelah dicek menggunakan fungsi `.isnull().sum()`, dan data duplikat menggunakan fungsi `.duplicated().sum()`, sehingga data sudah siap untuk digunakan lebih lanjut.

**Univariate Exploratory Data Analysis**
"""

# Melihat jumlah data
print('Jumlah data movie: ', len(movies.movieId.unique()))
print('Jumlah data ratings: ', len(ratings.userId.unique()))

"""**Movie Variabel**"""

movies.info()

movies.head()

print('Banyak data: ', len(movies.movieId.unique()))
genre_movie = movies['genres'].apply(lambda x: x.split('|')[0])
genre_movie = genre_movie.drop_duplicates()
genre_movie

"""Setelah mengetahui informasi dari dataset movie  pada kode diatas yang ternyata terdapat pipeline '|' untuk menambahkan beberapa nama genre, maka disini saya memisahkan beberapa genre yang berada pada baris yang sama"""

movies_exploded = movies.assign(genre=movies['genres'].str.split('|')).explode('genre')

genre_counts = movies_exploded['genre'].value_counts()

plt.figure(figsize=(10,6))
sns.barplot(x=genre_counts.values, y=genre_counts.index, palette='viridis')
plt.title('Jumlah Film per Genre')
plt.xlabel('Jumlah Film')
plt.ylabel('Genre')
for i, (count) in enumerate(genre_counts.values):
    plt.text(count + 2, i, str(count), va='center')
plt.tight_layout()
plt.show()

"""Pada kode diatas untuk melihat visualisasi diatas, terlihat bahwa Drama menjadi Genre dengan jumlah film terbanyak pada dataset ini"""

import re
def extract_year(title):
    match = re.search(r'\((\d{4})\)', title)
    if match:
        return int(match.group(1))
    return None

movies['year'] = movies['title'].apply(extract_year)

movies = movies.dropna(subset=['year'])

max_year = movies['year'].max()
movies_last_10_years = movies[movies['year'] >= max_year - 9]

year_counts = movies_last_10_years['year'].value_counts().sort_index()

plt.figure(figsize=(10,6))
sns.barplot(x=year_counts.index, y=year_counts.values, palette='crest')
plt.title('Distribusi Jumlah Film dalam 10 Tahun Terakhir')
plt.xlabel('Tahun Rilis')
plt.ylabel('Jumlah Film')
for i, count in enumerate(year_counts.values):
    plt.text(i, count + 1, str(count), ha='center', va='bottom')
plt.tight_layout()
plt.show()

"""Pada kode diatas untuk melihat visualisasi distribusi film dalam 10 tahun terakhir dengan terbanyak pada tahun 2009 dengan total 282 film, dan terkecil pada tahun 2018 dengan total 41 film

**Ratings Variabel**
"""

ratings.info()

ratings.head()

ratings.describe()

print('Jumlah userID: ', len(ratings.userId.unique()))
print('Jumlah movie: ', len(ratings.movieId.unique()))
print('Jumlah data rating: ', len(ratings))

"""Pada kode diatas untuk melihat informasi terkait dataset rating dengan fungsi `.info()`, melihat data teratas dengan fungsi `.head()`, melihat informasi statistika dataset rating dengan fungsi `.describe()`, dan melihat jumlah data pada dataset tersebut dengan fungsi `.len()`"""

rating_counts = ratings['rating'].value_counts().sort_index()

# Plot
plt.figure(figsize=(10,6))
sns.barplot(x=rating_counts.index.astype(str), y=rating_counts.values, palette='crest')
plt.title('Distribusi Jumlah Rating')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
for i, count in enumerate(rating_counts.values):
    plt.text(i, count + 1, str(count), ha='center', va='bottom')
plt.tight_layout()
plt.show()

"""Visualisasi distribusi jumlah rating terbanyak yaitu pada rating 4.0 dengan total 26818 rating, dan jumlah terkecil yaitu pada rating 0,5 dengan total 1370 rating"""

rating_counts = ratings['movieId'].value_counts().head(10)

# Gabungkan dengan judul film
top_movies = movies[movies['movieId'].isin(rating_counts.index)]
top_movies = top_movies.set_index('movieId').loc[rating_counts.index]
top_movies['rating_count'] = rating_counts.values

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(x=top_movies['rating_count'], y=top_movies['title'], palette='crest')
plt.title('10 Film dengan Jumlah Rating Terbanyak')
plt.xlabel('Jumlah Rating')
plt.ylabel('Judul Film')
for i, count in enumerate(top_movies['rating_count']):
    plt.text(count + 10, i, str(count), va='center')
plt.tight_layout()
plt.show()

"""pada kode tersebut untuk mendapatkan visualisasi 10 film dengan jumlah rating terbanyak, dengan urutan pertama yaitu film forrest gump (1994) dengan 329 rating, dan pada urutan 10 terdapat film schindler's list (1993)

**Data Preparation**

**Content Based Filtering**
"""

movieid = movies['movieId'].tolist()
movie_title = movies['title'].tolist()
movie_genre = movies['genres'].tolist()

print(len(movieid))
print(len(movie_title))
print(len(movie_genre))

"""Kode diatas untuk mengubah kolom tertentu menjadi list untuk mempermudah manipulasi dan pencocokan data, antara lain pada kolom `movieId` menjadi `movieid`, `title` menjadi `movie_title`, dan `genres` menjadi `movie_genre` pada dataset movies."""

movie_new = pd.DataFrame({
    'movieId': movieid,
    'movie_title': movie_title,
    'movie_genre': movie_genre
})
movie_new

"""selanjutnya membuat dictionary dari list untuk menentukan pasangan key-value pada data `movieid`, `movie_title`, dan `movie_genre` yang telah disiapkan sebelumnya, dengan nama dictionary `movie_new`."""

data = movie_new
data.sample(5)

"""Melihat sebagian data dari dataset besar untuk mempercepat eksplorasi dan pemodelan awal, dengan fungsi `sample(5)` untuk melihat 5 data pertama dari dataset.

**TF-IDF**
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data cuisine
tf.fit(data['movie_title'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['movie_title'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.movie_title
).sample(22, axis=1,replace=True).sample(10, axis=0)

"""pada tahap data preparation dengan TF-IDF dilakukan untuk mengubah teks (movie_title atau judul film) menjadi representasi numerik berbobot menggunakan TF-IDF, untuk menangkap pentingnya kata dalam konteks.
Output matriks TF-IDF `(9742, 9269)` yang berarti ada 9742 baris dan 9269 kolom kata unik.

**Cosine Similarity**
"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama resto
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['movie_title'], columns=data['movie_title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""pada tahap cosine similarity yang digunakan untuk menghitung derajat kesamaan (similarity degree) antar film. dan kemudian melihat hasil similarity tabel yang mempunyai kemiripan dengan fungsi `.sample()`

**Mendapatkan Rekomendasi**
"""

def recommendations(title, similarity_data=cosine_sim_df, items=data[['movie_genre', 'movie_title']], k=10):

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Tahap ini untuk mendapatkan rekomendasi film yang paling sesuai dengan film yang telah ditonton, menggunakan
fungsi `recommendations(title, similarity_data=cosine_sim_df, items=data[['movie_genre', 'movie_title']], k=10)` dengan penjelasan parameter sebagai berikut:
- title : Judul film (index kemiripan dataframe).
- Similarity_data : Dataframe mengenai similarity yang telah  didefinisikan pada tahap cosine similarity yang digunakan untuk menghitung derajat kesamaan (similarity degree) antar film
- Items : Nama dan fitur yang digunakan untuk mendefinisikan kemiripan, dalam hal ini adalah `movie_genre` dan `movie_title`.
- k : Banyak rekomendasi yang ingin diberikan.

Mencoba memasukkan nama film dan keluar hasil dengan nama film yang sama
"""

data[data.movie_title.eq('Toy Story (1995)')]

recommendations('Toy Story (1995)')

"""**Evaluation**

**Visualisasi Metrik**
"""

def precision_recall_content_based(input_movie, recommendations, k=10):
    input_tfidf = tf.transform([input_movie])
    rec_tfidf = tf.transform(recommendations)

    sim_scores = cosine_similarity(input_tfidf, rec_tfidf)[0]
    precision = sum(sim_scores[:k]) / k
    recall = sum(sim_scores[:k]) / sum(sim_scores) if sum(sim_scores) > 0 else 0
    return precision * 100, recall * 100

input_movies = ["Toy Story 2 (1999)", "Toy Story 3 (2010)"]
recommendation_results = {data: list(recommendations(data)['movie_title']) for data in input_movies}

for movie in input_movies:
    recommended_titles = recommendation_results[movie]
    precision, recall = precision_recall_content_based(movie, recommended_titles, k=5)
    print(f"Precision@5 for '{movie}': {precision:.2f}%")
    print(f"Recall@5 for '{movie}': {recall:.2f}%")

"""**Metrik Evaluasi** yang digunakan adalah Precision@K dan Recall@K, dengan formula sebagai berikut
- Precision@K = `True Positif / Total Rekomended Items`, metrik ini bekerja misalkan sistem merekomendasikan 5 film (K=5) ke pengguna, dan dari kelima film tersebut, 3 film ternyata disukai oleh pengguna (relevan), maka `3/5 = 0,6`, Precision@K berfokus pada akurasi dari top-K rekomendasi. Semakin tinggi nilainya, semakin banyak rekomendasi yang tepat sasaran dari total yang diberikan.
- Recall@K = `True Positif / Total Relevant Items`, metrik ini bekerja misalkan seorang pengguna menyukai 10 film secara keseluruhan, dan dari 5 film yang direkomendasikan (K=5), ada 3 yang memang dia sukai, maka `3/10 = 0,3`, Recall@K berfokus pada kelengkapan: seberapa banyak dari semua item relevan yang berhasil direkomendasikan.
**Interpretasi**
- `Toy Story 2 (1999)` :
    - Precision@5: 63.75% :
        Dari 5 rekomendasi teratas yang diberikan sistem kepada pengguna, sekitar 63.75% di antaranya benar-benar relevan (disukai atau cocok untuk pengguna). Artinya, sistem cukup akurat dalam memilih rekomendasi yang tepat untuk pengguna terkait film ini.
    - Recall@5: 65.74% :
        Dari seluruh film yang seharusnya relevan untuk pengguna (misalnya berdasarkan preferensi atau riwayat mereka), sistem berhasil menemukan dan menyarankan 65.74% di antaranya dalam 5 rekomendasi teratas. Artinya, sistem cukup komprehensif dalam menemukan film yang relevan meskipun hanya memberikan 5 rekomendasi.
- `Toy Story 3 (2010)` :
    - Precision@5: 62.76% :
        Dari 5 film yang direkomendasikan terkait Toy Story 3 (2010), 62.76% di antaranya sesuai dengan selera atau preferensi pengguna. Nilai ini sedikit lebih rendah dibanding Toy Story 2, tapi masih cukup baik.
    - Recall@5: 67.86% :
        Sistem berhasil mencakup 67.86% dari semua film yang seharusnya direkomendasikan kepada pengguna terkait Toy Story 3. Ini menunjukkan sistem agak lebih baik dalam cakupan (recall) dibanding Toy Story 2, walaupun presisinya sedikit lebih rendah.

Dengan evaluasi tersebut, teknik Content Based Filtering sudah bisa memberikan rekomendasi film yang dipersonalisasi untuk pengguna.

**Collaborative Filtering**
"""

# Import library
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""**Data Preparation**"""

df = ratings
df

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['userId'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah userID menjadi list tanpa nilai yang sama
movie_ids = df['movieId'].unique().tolist()
print('judul movie: ', movie_ids)

# Melakukan encoding userID
judul_to_judul_encoded = {x: i for i, x in enumerate(movie_ids)}
print('encoded judul movie : ', judul_to_judul_encoded)

# Melakukan proses encoding angka ke ke userID
judul_encoded_to_judul = {i: x for i, x in enumerate(movie_ids)}
print('encoded angka ke cluster: ', judul_encoded_to_judul)

"""pada kode diatas adalah untuk mengubah ID user dan movieId menjadi angka untuk dapat diproses oleh model."""

# Mapping userID ke dataframe user
df['user'] = df['userId'].map(user_to_user_encoded)

# Mapping movieId ke dataframe judul
df['movie'] = df['movieId'].map(judul_to_judul_encoded)

"""Tahap tersebut untuk melakukan mapping atau mengganti nilai userID di DataFrame df menjadi nilai numerik (encoded).
Dengan fungsi `.map()`.
"""

# Mendapatkan jumlah user
num_user = len(user_to_user_encoded)
print(num_user)

num_judul = len(judul_to_judul_encoded)
print(num_judul)

# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['rating'])

# Nilai maksimal rating
max_rating = max(df['rating'])

print('Number of Track: {}, Number of Title: {}, Min Rating: {}, Max Rating: {}'.format(
    num_user, num_judul, min_rating, max_rating
))

"""Pada tahap tersebut cek beberapa hal dalam data seperti jumlah user, jumlah judul film, dan mengubah nilai rating menjadi float. Untuk mengecek jumlah menggunakan fungsi `.len()`, selanjutnya untuk melihat minimal data menggunakan fungsi `.min()`, dan maximal data menggunakan fungsi `.max()` dan untuk mengubah nilai rating menjadi float menggunakan fungsi `.astype(np.float32)`.

**Data Splitting**
"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""Pada tahap tersebut kita akan membagi data menjadi 2 bagian yaitu data latih dan validasi untuk evaluasi model yang objektif. Pada proyek ini data dibagi menjadi `80%` untuk `data latih` dan `20%` untuk `data validasi`."""

# Membuat variabel x untuk mencocokkan data user dan movie menjadi satu value
x = df[['user', 'movie']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_user, num_judul, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_user = num_user
    self.num_judul = num_judul
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_user,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_user, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movie
        num_judul,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_judul, 1) # layer embedding movie bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    judul_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    judul_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_track_judul = tf.tensordot(user_vector, judul_vector, 2)

    x = dot_track_judul + user_bias + judul_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""Pada tahap modeling ini menggunakan RecommenderNet (Custom Keras Model) dengan: Embedding layer untuk user dan film. Dot product untuk memprediksi rating. Proses compile pada model dengan binary crossentropy sebagai loss function, adam sebagai optimizer, dan RMSE sebagai metrik dari model."""

model = RecommenderNet(num_user, num_judul, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 30,
    validation_data = (x_val, y_val)
)

"""**Evaluation**

**Visualisasi Metrik**
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

movie_df = movie_new
df = pd.read_csv('ratings.csv')

# Mengambil sample user
user_id = df.userId.sample(1).iloc[0]
movie_watched_by_user = df[df.userId == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
movie_not_watched = movie_df[~movie_df['movieId'].isin(movie_watched_by_user.movieId.values)]['movieId']
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(judul_to_judul_encoded.keys()))
)

movie_not_watched = [[judul_to_judul_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    judul_encoded_to_judul.get(movie_not_watched[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Movie with high ratings from user')
print('----' * 8)

top_movie_user = (
    movie_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

movie_df_rows = movie_df[movie_df['movieId'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.movie_title, ':', row.movie_genre)

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

recommended_movie = movie_df[movie_df['movieId'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.movie_title, ':', row.movie_genre)

"""**Metrik Evaluasi** yang digunakan adalah RMSE, metrik RMSE mengukur seberapa jauh prediksi model dari nilai sebenarnya, dengan memberi penalti lebih besar pada kesalahan yang besar. Semakin kecil RMSE, semakin baik performa model.
**Interpretasi Grafik RMSE**
- RMSE pada data train dan test sama-sama turun secara signifikan, menunjukkan bahwa model belajar dengan baik dan memperbaiki kesalahan prediksinya.
- RMSE pada data train terus menurun stabil hingga akhir pelatihan (~0.175), yang berarti model semakin cocok terhadap data pelatihan.
- Nilai akhir RMSE berada pada 0.1774, mengindikasikan error prediksi yang relatif rendah.

Dengan hasil evaluasi tersebut, teknik Collaborative Filtering sudah bisa menyesuaikan preferensi pengguna dan memberikan rekomendasi film yang belum pernah ditonton dengan rating tertinggi.
"""